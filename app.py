# app.py -- Streamlit Web UI for your dataset-merge pipeline (+ ER/MVI + Validation)
import os, sys, json, time, zipfile, io, csv, subprocess
from pathlib import Path
import streamlit as st
from subprocess import CalledProcessError

ROOT = Path(__file__).parent.resolve()
PIPELINE = ROOT / "the_pipeline_v2.py"        # ä½ çš„ç°æœ‰è„šæœ¬
VALIDATOR = ROOT / "validate_output_dataset.py"
DEFAULT_DATAS_GLOB = "datalake/*.{csv,json,ndjson,jsonl,xlsx,xls}"
RESULTS_DIR = ROOT / "results_webui"
RESULTS_DIR.mkdir(exist_ok=True)

st.set_page_config(page_title="DataSearchTool Web UI", layout="wide")
st.title("ğŸ§  DataSearchTool â€“ è‡ªç„¶è¯­è¨€æ‰¾æ•° Â· è‡ªåŠ¨åˆå¹¶ Â· ER/MVI Â· æ ¡éªŒ")

# ---------------------- Sidebar: åŸºç¡€é…ç½® ----------------------
st.sidebar.header("âš™ï¸ åŸºç¡€é…ç½®")
api_base = st.sidebar.text_input("LLM API Base URL", value="https://goapi.gptnb.ai/v1/chat/completions")
api_key  = st.sidebar.text_input("LLM API Keyï¼ˆä¼˜å…ˆç”¨ç¯å¢ƒå˜é‡ GPTNB_API_KEYï¼‰", type="password", value=os.getenv("GPTNB_API_KEY", ""))
model    = st.sidebar.text_input("æ¨¡å‹åï¼ˆleft/map/join å…±ç”¨ï¼‰", value="gpt-4o-mini")
max_conc = st.sidebar.number_input("max_concurrency", 1, 16, 4)
graph_out = st.sidebar.checkbox("å¯¼å‡º join graphï¼ˆ.dotï¼‰ï¼ˆè‹¥è„šæœ¬æ”¯æŒï¼‰", value=False)

# ---------------------- Sidebar: å¤–éƒ¨ API å¢å¼º ----------------------
st.sidebar.header("ğŸ” å¤–éƒ¨ API å¢å¼ºï¼ˆå¯é€‰ï¼‰")
st.sidebar.caption("è¿™äº›å¯†é’¥å°†æ³¨å…¥åˆ°å­è¿›ç¨‹ç¯å¢ƒå˜é‡ï¼ŒER/MVI ä¼šè‡ªåŠ¨æŒ‰é¢†åŸŸ/æ•°æ®é€‰æ‹©åˆé€‚çš„å¤–éƒ¨æºã€‚")
k_user = st.sidebar.text_input("Kaggle Usernameï¼ˆKAGGLE_USERNAMEï¼‰", value=os.getenv("KAGGLE_USERNAME", ""))
k_key  = st.sidebar.text_input("Kaggle API Keyï¼ˆKAGGLE_KEYï¼‰", type="password", value=os.getenv("KAGGLE_KEY", ""))
omdb_key = st.sidebar.text_input("OMDb API Keyï¼ˆOMDB_API_KEYï¼‰", type="password", value=os.getenv("OMDB_API_KEY", ""))
tmdb_key = st.sidebar.text_input("TMDb API Keyï¼ˆTMDB_API_KEYï¼‰", type="password", value=os.getenv("TMDB_API_KEY", ""))

st.sidebar.divider()
st.sidebar.header("ğŸ§© ER / MVI é€‰é¡¹")
enable_er_mvi = st.sidebar.checkbox("å¯ç”¨ ER/MVIï¼ˆè‡ªåŠ¨åŸŸè¯†åˆ« + Wikipedia/Kaggle/â€¦ï¼‰", value=True)
er_sample_rows = st.sidebar.slider("ER/MVI æŠ½æ ·è¡Œæ•°ï¼ˆé¿å…å¤–éƒ¨è¯·æ±‚è¿‡å¤§ï¼‰", min_value=50, max_value=1000, value=200, step=50)
st.sidebar.caption("ğŸ‘‰ æœªé…ç½®å¯†é’¥æ—¶ä¼šä»…ä½¿ç”¨æ— éœ€å¯†é’¥çš„æºï¼ˆä¾‹å¦‚ Wikipediaï¼‰ï¼Œæˆ–è·³è¿‡è¯¥æºã€‚")

# ---------------------- Queries è¾“å…¥ ----------------------
st.subheader("ğŸ“ è‡ªç„¶è¯­è¨€é—®é¢˜ï¼ˆæ¯è¡Œä¸€æ¡ï¼‰")
text_queries = st.text_area(
    "ç¤ºä¾‹ï¼š\n"
    "I need a movie dataset with title, year, director, genre and budget\n"
    "Get a dataset for predicting movie revenue using numeric features\n"
    "Find stock OHLC tables for AAPL with dates and volumes",
    height=150
)

st.write("æˆ–ä¸Šä¼  `queries_sig.csv`ï¼ˆéœ€åŒ…å«åˆ—å **NL Questions**ï¼‰ï¼š")
csv_file = st.file_uploader("ä¸Šä¼  queries CSV", type=["csv"])

queries = []
if csv_file is not None:
    content = csv_file.read().decode("utf-8", errors="ignore").splitlines()
    reader = csv.DictReader(content)
    for row in reader:
        if "NL Questions" in row and row["NL Questions"].strip():
            queries.append(row["NL Questions"].strip())

extra_lines = [q.strip() for q in text_queries.splitlines() if q.strip()]
queries.extend(extra_lines)
queries = [q for q in queries if q]

# ---------------------- ä¸Šä¼ æ•°æ®ï¼ˆå¯é€‰ï¼‰ ----------------------
st.subheader("ğŸ“‚ ä¸Šä¼ æ•°æ®ï¼ˆå¯é€‰ï¼‰")
st.caption("å¯é€‰ï¼›ä¹Ÿå¯ä»¥ç›´æ¥ç”¨ç£ç›˜ä¸Šçš„ `datalake/*.{csv,json,ndjson,jsonl,xlsx,xls}`ã€‚ä¸Šä¼ æ–‡ä»¶å°†ä¿å­˜åˆ° `datas/` å‚ä¸æŠ“å–ã€‚")
uploads = st.file_uploader(
    "æ‹–å…¥å¤šä¸ª CSV/JSON/NDJSON/JSONL/Excel",
    type=["csv", "json", "ndjson", "jsonl", "xlsx", "xls"],
    accept_multiple_files=True
)
if uploads:
    data_dir = ROOT / "datas"
    data_dir.mkdir(exist_ok=True)
    for f in uploads:
        (data_dir / f.name).write_bytes(f.read())
    st.success(f"å·²ä¿å­˜åˆ° {data_dir}ï¼ˆå…± {len(uploads)} ä¸ªæ–‡ä»¶ï¼‰")

# ---------------------- è¿è¡ŒæŒ‰é’® ----------------------
run = st.button("ğŸš€ å¼€å§‹è¿è¡Œ", type="primary", disabled=(len(queries) == 0))
log_container = st.container()

def make_slug(idx: int, question: str) -> str:
    base = ''.join(ch if ch.isalnum() else '_' for ch in question.lower())[:40] or 'q'
    return f"q{idx}_{base}"

def run_one_query(idx: int, question: str) -> dict:
    """
    ä¸ºå•æ¡ query ç”Ÿæˆä¸´æ—¶é…ç½®ï¼Œè°ƒç”¨ the_pipeline_v2.pyï¼Œéšåè°ƒç”¨ validate_output_dataset.pyã€‚
    """
    slug = make_slug(idx, question)
    cfg_path = RESULTS_DIR / f"{slug}.json"
    out_csv  = RESULTS_DIR / f"{slug}_merged.csv"
    dot_path = RESULTS_DIR / f"{slug}_graph.dot" if graph_out else ""
    meta_path = out_csv.with_suffix(".meta.json")

    # ç»„è£…é»˜è®¤æŠ“å–ï¼šdatas + datalake
    datasets_globs = [
        "datas/**/*.{csv,json,ndjson,jsonl,xlsx,xls}",
        "datalake/**/*.{csv,json,ndjson,jsonl,xlsx,xls}",
    ]

    cfg = {
        "question": question,
        "datasets": datasets_globs,
        "out": str(out_csv),
        "graph_out": str(dot_path),
        "left_model": model,
        "map_model": model,
        "join_model": model,
        "max_concurrency": int(max_conc),
        "api_base": api_base,
        "api_key": api_key or os.getenv("GPTNB_API_KEY", ""),
        "er_mvi_sample_rows": int(er_sample_rows),
        "enable_er_mvi": bool(enable_er_mvi),
    }
    cfg_path.write_text(json.dumps(cfg, ensure_ascii=False, indent=2), encoding="utf-8")

    # æ„å»ºå­è¿›ç¨‹ç¯å¢ƒï¼šLLM + å¤–éƒ¨å¢å¼º
    env_for_child = os.environ.copy()
    if api_key:
        env_for_child["GPTNB_API_KEY"] = api_key
    if k_user: env_for_child["KAGGLE_USERNAME"] = k_user
    if k_key:  env_for_child["KAGGLE_KEY"] = k_key
    if omdb_key: env_for_child["OMDB_API_KEY"] = omdb_key
    if tmdb_key: env_for_child["TMDB_API_KEY"] = tmdb_key
    env_for_child["ER_MVI_ENABLED"] = "1" if enable_er_mvi else "0"
    env_for_child["ER_MVI_SAMPLE_ROWS"] = str(er_sample_rows)

    # è¿è¡Œä¸»ç®¡é“
    t0 = time.time()
    proc = subprocess.run(
        [sys.executable, str(PIPELINE), str(cfg_path)],
        capture_output=True,
        text=True,
        env=env_for_child,
        cwd=str(ROOT),
    )
    elapsed = time.time() - t0

    out_csv_exists = out_csv.exists()
    meta_exists = meta_path.exists()
    dot_exists = (Path(dot_path).exists() if graph_out and dot_path else False)

    summary = {
        "question": question,
        "slug": slug,
        "elapsed_s": round(elapsed, 2),
        "returncode": proc.returncode,
        "stdout": proc.stdout,
        "stderr": proc.stderr,
        "out_csv": str(out_csv) if out_csv_exists else "",
        "graph_dot": str(dot_path) if dot_exists else "",
        "meta_json": str(meta_path) if meta_exists else "",
        "entity_report": "",
        "validation_report": "",
        "validation_stdout": "",
        "validation_stderr": "",
    }

    # ä» meta è¯»å– entity_reportï¼ˆè‹¥å­˜åœ¨ï¼‰
    if meta_exists:
        try:
            meta = json.loads(meta_path.read_text("utf-8"))
            erp = meta.get("entity_report", "")
            if erp:
                erp_path = Path(erp)
                # è‹¥æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œåˆ™ç”¨ out_csv çš„åŒç›®å½•è§£æï¼›è‹¥ out_csv ä¸å­˜åœ¨åˆ™è·³è¿‡
                if not erp_path.is_absolute() and out_csv_exists:
                    erp_path = out_csv.with_name(out_csv.stem + "_entity_report.csv")
                if erp_path.exists():
                    summary["entity_report"] = str(erp_path)
        except Exception:
            pass

    # å…œåº•ï¼šä»…å½“ out_csv å­˜åœ¨æ—¶ï¼Œæ‰æ ¹æ® out_csv æ¨æ–­ entity_report è·¯å¾„
    if not summary["entity_report"] and out_csv_exists:
        guess_rep = out_csv.with_name(out_csv.stem + "_entity_report.csv")
        if guess_rep.exists():
            summary["entity_report"] = str(guess_rep)

    # â€”â€” ç”Ÿæˆ validation_report.csv ï¼ˆç‹¬ç«‹è„šæœ¬è°ƒç”¨ï¼Œä¿æŒè§£è€¦ï¼‰â€”â€”
    report_path = RESULTS_DIR / f"{slug}_validation_report.csv"
    if out_csv_exists and VALIDATOR.exists():
        try:
            vr = subprocess.run(
                [
                    sys.executable, str(VALIDATOR),
                    "--csv", str(out_csv),
                    "--templates", str(ROOT / "templates"),
                    "--out", str(report_path),
                ],
                capture_output=True,
                text=True,
                env=env_for_child,
                cwd=str(ROOT),
            )
            if vr.returncode == 0 and report_path.exists():
                summary["validation_report"] = str(report_path)
                summary["validation_stdout"] = vr.stdout[-2000:]
            else:
                summary["validation_stdout"] = vr.stdout[-2000:]
                summary["validation_stderr"] = vr.stderr[-2000:]
        except CalledProcessError as e:
            summary["validation_stderr"] = f"CalledProcessError: {e}"
    else:
        if not VALIDATOR.exists():
            summary["validation_stderr"] = "validate_output_dataset.py ä¸å­˜åœ¨ï¼Œè·³è¿‡éªŒè¯ã€‚"

    return summary



if run:
    st.toast(f"å¼€å§‹è¿è¡Œï¼ˆå…± {len(queries)} æ¡ï¼‰", icon="âœ…")
    results = []
    progress = st.progress(0.0)

    for i, q in enumerate(queries, 1):
        with log_container.expander(f"[{i}/{len(queries)}] {q}", expanded=True):
            res = run_one_query(i, q)
            results.append(res)

            if res["returncode"] == 0:
                st.success(f"å®Œæˆï¼Œè€—æ—¶ {res['elapsed_s']} s")

                # åˆå¹¶è¾“å‡º
                if res["out_csv"]:
                    st.write(f"åˆå¹¶è¾“å‡ºï¼š`{res['out_csv']}`")
                    st.download_button("ä¸‹è½½åˆå¹¶ CSV", data=Path(res["out_csv"]).read_bytes(),
                                       file_name=Path(res["out_csv"]).name, mime="text/csv")

                # Meta
                if res["meta_json"]:
                    try:
                        meta = json.loads(Path(res["meta_json"]).read_text("utf-8"))
                        with st.expander("Metaï¼ˆmatched_files / diagnostics / enrichmentï¼‰", expanded=False):
                            st.json(meta)
                    except Exception as e:
                        st.caption(f"è¯»å– meta å¤±è´¥ï¼š{e}")

                # Join Graph
                if res["graph_dot"]:
                    st.write(f"Join Graphï¼š`{res['graph_dot']}`ï¼ˆ.dotï¼‰")
                    st.download_button("ä¸‹è½½ DOT", data=Path(res["graph_dot"]).read_bytes(),
                                       file_name=Path(res["graph_dot"]).name, mime="text/vnd.graphviz")

                # ER/MVI æŠ¥å‘Š
                if res.get("entity_report"):
                    rp = Path(res["entity_report"])
                    if rp.exists():
                        st.write(f"ER/MVI æ ·æœ¬æŠ¥å‘Šï¼š`{rp}`")
                        st.download_button("ä¸‹è½½ ER/MVI æŠ¥å‘Šï¼ˆCSVï¼‰",
                                           data=rp.read_bytes(),
                                           file_name=rp.name, mime="text/csv")

                # éªŒè¯æŠ¥å‘Š
                if res.get("validation_report"):
                    rp = Path(res["validation_report"])
                    st.write(f"éªŒè¯æŠ¥å‘Šï¼š`{rp}`")
                    st.download_button("ä¸‹è½½ validation_report.csv",
                                       data=rp.read_bytes(),
                                       file_name=rp.name, mime="text/csv")
                elif res.get("validation_stdout") or res.get("validation_stderr"):
                    st.caption("validation output:")
                    if res.get("validation_stdout"):
                        st.code(res["validation_stdout"], language="bash")
                    if res.get("validation_stderr"):
                        st.code(res["validation_stderr"], language="bash")
            else:
                st.error(f"å¤±è´¥ï¼ˆè€—æ—¶ {res['elapsed_s']} sï¼‰")
                if res["stderr"]:
                    st.code(res["stderr"][-2000:], language="bash")

            st.caption("stdout æ‘˜è¦ï¼š")
            st.code(res["stdout"][-2000:], language="bash")
        progress.progress(i / len(queries))

    # æ‰“åŒ…æ‰€æœ‰ç»“æœä¸º zip æä¾›ä¸‹è½½
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
        for p in RESULTS_DIR.glob("*"):
            if p.is_file():
                zf.write(p, arcname=p.name)
    st.success("å…¨éƒ¨å®Œæˆ ğŸ‰")
    st.download_button("æ‰“åŒ…ä¸‹è½½æ‰€æœ‰ç»“æœï¼ˆZIPï¼‰", data=buf.getvalue(), file_name="results_webui.zip", mime="application/zip")
else:
    st.info("å¡«å¥½é—®é¢˜åç‚¹å‡»ä¸Šé¢çš„ **å¼€å§‹è¿è¡Œ**ã€‚")
