# llm_entity_resolver.py

from typing import List, Dict, Any, Tuple
import polars as pl
import asyncio
import json
from difflib import SequenceMatcher
from pathlib import Path


class LLMEntityResolver:

    def __init__(self, llm_client, cache=None):
        self.llm = llm_client
        self.cache = cache or {}
        self.accepted_edges = []  # [(idx1, idx2, confidence)]
        self.last_key_cols = []  # è®°ä½æœ¬æ¬¡ERè¯†åˆ«åˆ°çš„å…³é”®åˆ—ï¼Œä¾›DOTæ ‡ç­¾ç”¨

    async def resolve(
            self,
            df: pl.DataFrame,
            entity_type: str = "entity",
            max_comparisons: int = 200,  # â† ä¼˜åŒ–ï¼šä» 500 å‡å°‘åˆ° 200
            confidence_threshold: float = 0.8
    ) -> Tuple[pl.DataFrame, pl.DataFrame]:
        """
        ä¸»å…¥å£: å¯¹æ•°æ®é›†è¿›è¡Œå®ä½“è§£æï¼ˆä¼˜åŒ–ç‰ˆï¼šæ›´å¿«çš„é»˜è®¤å‚æ•°ï¼‰

        Args:
            df: è¾“å…¥æ•°æ®
            entity_type: å®ä½“ç±»å‹æè¿° (å¦‚ "movie", "product")
            max_comparisons: æœ€å¤§LLMæ¯”è¾ƒæ¬¡æ•°ï¼ˆé»˜è®¤ 200ï¼Œé™ä½ä»¥æå‡é€Ÿåº¦ï¼‰
            confidence_threshold: åˆå¹¶é˜ˆå€¼ï¼ˆé»˜è®¤ 0.8ï¼‰

        Returns:
            (resolved_df, report_df)
        """

        print(f"[LLM-ER] Resolving {entity_type} entities...")

        # Step 1: è¯†åˆ«å…³é”®åˆ—
        print("[LLM-ER] Step 1: Identifying key columns...")
        key_cols = await self.identify_key_columns(df, entity_type)
        print(f"[LLM-ER] Key columns: {key_cols}")

        # Step 2: å€™é€‰å¯¹ç”Ÿæˆï¼ˆå¯å‘å¼ï¼‰
        print("[LLM-ER] Step 2: Generating candidate pairs...")
        candidates = self.generate_candidates(df, key_cols, max_comparisons)
        print(f"[LLM-ER] Found {len(candidates)} candidate pairs")

        # â† ä¼˜åŒ–ï¼šå¦‚æœå€™é€‰å¯¹å¤ªå¤šï¼Œå‘å‡ºè­¦å‘Šå¹¶å»ºè®®å‡å°‘
        if len(candidates) > 100:
            print(f"[LLM-ER] âš ï¸  Large number of candidates ({len(candidates)}) may slow down execution")
            print(f"[LLM-ER] ğŸ’¡ Tip: Set ER_ENABLED=0 or reduce sample_n to speed up")

        # Step 3: LLMåˆ¤æ–­
        print("[LLM-ER] Step 3: LLM-based comparison...")
        merge_graph = await self.compare_pairs(
            df, candidates, entity_type, confidence_threshold
        )

        # Step 4: åˆå¹¶
        print("[LLM-ER] Step 4: Merging entities...")
        resolved_df = self.merge_entities(df, merge_graph)

        # Step 5: ç”ŸæˆæŠ¥å‘Š
        report_df = self.generate_report(df, resolved_df, merge_graph)

        return resolved_df, report_df

    async def identify_key_columns(
            self,
            df: pl.DataFrame,
            entity_type: str
    ) -> List[str]:
        """
        è®©LLMè¯†åˆ«å“ªäº›åˆ—æ˜¯å…³é”®æ ‡è¯†åˆ—
        """

        # ç¼“å­˜key
        cache_key = f"key_cols_{hash(str(df.columns))}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        # å‡†å¤‡schemaä¿¡æ¯
        schema_info = {}
        for col in df.columns[:20]:  # é™åˆ¶åˆ—æ•°
            try:
                schema_info[col] = {
                    "dtype": str(df[col].dtype),
                    "sample": df[col].head(3).drop_nulls().to_list()[:3],
                    "null_pct": round(df[col].null_count() / len(df) * 100, 1)
                }
            except:
                continue

        prompt = f"""You are analyzing a dataset of {entity_type} records.

Schema:
{json.dumps(schema_info, indent=2)}

Identify 1-3 columns that are KEY IDENTIFIERS for determining if two records are the same {entity_type}.

Key columns typically:
- Uniquely identify entities (names, titles, IDs)
- Have low null percentage
- Are string or categorical types

Return ONLY a JSON array of column names:
["col1", "col2"]"""

        try:
            response = await self.llm.chat("gpt-4o-mini", [
                {"role": "user", "content": prompt}
            ])

            # ä½¿ç”¨å¥å£®çš„JSONæå–æ–¹æ³•ï¼ˆä¸are_same_entityä¸€è‡´ï¼‰
            key_cols = self._extract_json_list_from_response(response)

            # éªŒè¯åˆ—å
            key_cols = [c for c in key_cols if c in df.columns]

            if not key_cols:
                # å›é€€: é€‰æ‹©ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—
                key_cols = [c for c in df.columns
                            if df[c].dtype == pl.Utf8][:1]

            self.cache[cache_key] = key_cols
            return key_cols

        except Exception as e:
            print(f"[LLM-ER] Error in identify_key_columns: {e}")
            # å›é€€ç­–ç•¥
            return [c for c in df.columns if df[c].dtype == pl.Utf8][:1]

    def _extract_json_list_from_response(self, text: str) -> list:
        """ä» LLM å“åº”ä¸­æå– JSON æ•°ç»„ï¼ˆå¤„ç† markdown ä»£ç å—ç­‰ï¼‰"""
        import re

        if not text or not text.strip():
            raise ValueError("Empty response")

        text = text.strip()

        # å°è¯•1: ç›´æ¥è§£æ
        try:
            result = json.loads(text)
            if isinstance(result, list):
                return result
            # å¦‚æœæ˜¯å­—å…¸ä½†æœ‰æ•°ç»„å­—æ®µï¼Œå°è¯•æå–
            if isinstance(result, dict):
                for key in ['columns', 'key_columns', 'identifiers', 'fields']:
                    if key in result and isinstance(result[key], list):
                        return result[key]
        except:
            pass

        # å°è¯•2: æå– markdown ä»£ç å—ä¸­çš„ JSON
        # ```json [...] ``` æˆ– ```[...]```
        match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass

        # å°è¯•3: æå–ç¬¬ä¸€ä¸ª [...] å—
        match = re.search(r'\[.*?\]', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(0))
            except:
                pass

        # å°è¯•4: å¦‚æœåŒ…å«åˆ—åå…³é”®å­—ï¼Œå°è¯•æ‰‹åŠ¨è§£æ
        # åŒ¹é… "col1", "col2", "col3" è¿™æ ·çš„æ¨¡å¼
        matches = re.findall(r'"([^"]+)"', text)
        if matches:
            return matches

        # å°è¯•5: åŒ¹é…å•å¼•å·
        matches = re.findall(r"'([^']+)'", text)
        if matches:
            return matches

        raise ValueError(f"Could not extract JSON array from response: {text[:200]}")

    def generate_candidates(
            self,
            df: pl.DataFrame,
            key_cols: List[str],
            max_pairs: int
    ) -> List[Tuple[int, int]]:
        """
        ç”Ÿæˆå€™é€‰é‡å¤å¯¹ï¼ˆä¼˜åŒ–ç‰ˆï¼šå‡å°‘LLMè°ƒç”¨ï¼‰

        ç­–ç•¥:
        1. Blocking by first character
        2. æé«˜ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.5 â†’ 0.7ï¼‰
        3. ä¸¥æ ¼é™åˆ¶ç»„å†…æ¯”è¾ƒæ•°é‡
        4. é™åˆ¶æ€»å€™é€‰å¯¹æ•°é‡
        """

        if not key_cols:
            return []

        candidates = []
        primary_col = key_cols[0]

        # Blocking: æŒ‰é¦–å­—æ¯åˆ†ç»„
        try:
            # æ·»åŠ é¦–å­—æ¯åˆ—ï¼ˆå…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²å¤„ç†æ•°å€¼åˆ—ï¼‰
            df_with_block = df.with_columns([
                pl.col(primary_col)
                .cast(pl.Utf8)  # â† ä¿®å¤ï¼šå…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                .str.slice(0, 1)
                .str.to_lowercase()
                .alias("_block_key")
            ])

            # æŒ‰blockåˆ†ç»„
            for block_key, group_df in df_with_block.group_by("_block_key"):
                if len(group_df) <= 1:
                    continue

                indices = group_df.select(pl.arange(0, pl.count()).alias("idx"))["idx"].to_list()

                # ç»„å†…ä¸¤ä¸¤æ¯”è¾ƒï¼ˆä¸¥æ ¼é™åˆ¶æ•°é‡ï¼‰
                for i, idx1 in enumerate(indices):
                    # â† ä¼˜åŒ–ï¼šæ¯ä¸ªå…ƒç´ æœ€å¤šæ¯”è¾ƒ 5 ä¸ªï¼ˆä¹‹å‰æ˜¯ 10 ä¸ªï¼‰
                    for idx2 in indices[i + 1:min(i + 6, len(indices))]:
                        # ç®€å•é¢„ç­›é€‰ï¼ˆæé«˜é˜ˆå€¼ï¼‰
                        val1 = str(df[primary_col][idx1] or "")
                        val2 = str(df[primary_col][idx2] or "")

                        # â† ä¼˜åŒ–ï¼šæé«˜ç›¸ä¼¼åº¦é˜ˆå€¼ 0.5 â†’ 0.7
                        if self.quick_similarity(val1, val2) > 0.7:
                            candidates.append((idx1, idx2))

                        # â† ä¼˜åŒ–ï¼šæ›´æ—©é€€å‡º
                        if len(candidates) >= max_pairs:
                            print(f"[LLM-ER] Reached max_pairs limit: {max_pairs}")
                            return candidates[:max_pairs]

        except Exception as e:
            print(f"[LLM-ER] Error in generate_candidates: {e}")
            # å›é€€: æ›´ä¿å®ˆçš„é‡‡æ ·
            n = min(len(df), 50)  # â† ä¼˜åŒ–ï¼šä» 100 å‡å°‘åˆ° 50
            for i in range(n):
                for j in range(i + 1, min(i + 4, n)):  # â† ä¼˜åŒ–ï¼šä» 6 å‡å°‘åˆ° 4
                    candidates.append((i, j))

        return candidates[:max_pairs]

    def quick_similarity(self, s1: str, s2: str) -> float:
        """å¿«é€Ÿç›¸ä¼¼åº¦ï¼ˆç”¨äºé¢„ç­›é€‰ï¼‰"""
        return SequenceMatcher(None, s1.lower(), s2.lower()).ratio()

    async def compare_pairs(
            self,
            df: pl.DataFrame,
            candidates: List[Tuple[int, int]],
            entity_type: str,
            threshold: float
    ) -> Dict[int, int]:
        """
        ç”¨LLMæ¯”è¾ƒå€™é€‰å¯¹

        è¿”å›: merge_graph {from_idx: to_idx}
        """

        merge_graph = {}

        # æ‰¹é‡å¤„ç†ï¼ˆå‡å°‘APIè°ƒç”¨ï¼‰
        batch_size = 10
        for i in range(0, len(candidates), batch_size):
            batch = candidates[i:i + batch_size]

            tasks = [
                self.are_same_entity(
                    df.row(idx1, named=True),
                    df.row(idx2, named=True),
                    entity_type
                )
                for idx1, idx2 in batch
            ]

            results = await asyncio.gather(*tasks)

            for (idx1, idx2), (same, confidence) in zip(batch, results):
                if same and confidence >= threshold:
                    # Union-Find: åˆå¹¶åˆ°åŒä¸€ä¸ªcluster
                    root1 = self.find_root(merge_graph, idx1)
                    root2 = self.find_root(merge_graph, idx2)
                    if root1 != root2:
                        merge_graph[root2] = root1

        return merge_graph

    def find_root(self, merge_graph: Dict[int, int], idx: int) -> int:
        """Union-Find: æ‰¾æ ¹èŠ‚ç‚¹"""
        if idx not in merge_graph:
            return idx
        root = idx
        while root in merge_graph:
            root = merge_graph[root]
        # è·¯å¾„å‹ç¼©
        while idx != root:
            next_idx = merge_graph[idx]
            merge_graph[idx] = root
            idx = next_idx
        return root

    async def are_same_entity(
            self,
            entity1: Dict[str, Any],
            entity2: Dict[str, Any],
            entity_type: str
    ) -> Tuple[bool, float]:
        """
        æ ¸å¿ƒ: ç”¨LLMåˆ¤æ–­ä¸¤ä¸ªå®ä½“æ˜¯å¦ç›¸åŒ
        """
        import re

        # ç¼“å­˜
        cache_key = f"{hash(str(sorted(entity1.items())))}_{hash(str(sorted(entity2.items())))}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        # ç®€åŒ–æ˜¾ç¤ºï¼ˆåªæ˜¾ç¤ºéç©ºå­—æ®µï¼‰
        e1_clean = {k: v for k, v in entity1.items() if v is not None}
        e2_clean = {k: v for k, v in entity2.items() if v is not None}

        prompt = f"""Are these two {entity_type} records the same entity?

Record 1: {json.dumps(e1_clean)}
Record 2: {json.dumps(e2_clean)}

Consider variations like:
- Typos/spelling
- Missing words
- Abbreviations  
- Different formats

Respond ONLY with valid JSON (no markdown, no extra text):
{{"same": true/false, "confidence": 0.0-1.0}}"""

        try:
            response = await self.llm.chat("gpt-4o-mini", [
                {"role": "user", "content": prompt}
            ])

            # å¥å£®çš„ JSON æå–
            result = self._extract_json_from_response(response)

            same = result.get("same", False)
            confidence = float(result.get("confidence", 0.0))

            self.cache[cache_key] = (same, confidence)
            return same, confidence

        except Exception as e:
            print(f"[LLM-ER] Error in are_same_entity: {e}")
            print(f"[LLM-ER] Response was: {response[:200] if 'response' in locals() else 'N/A'}")
            # å›é€€: ä¿å®ˆç­–ç•¥
            return False, 0.0

    def _extract_json_from_response(self, text: str) -> dict:
        """ä» LLM å“åº”ä¸­æå– JSONï¼ˆå¤„ç† markdown ä»£ç å—ç­‰ï¼‰"""
        import re

        if not text or not text.strip():
            raise ValueError("Empty response")

        text = text.strip()

        # å°è¯•1: ç›´æ¥è§£æ
        try:
            return json.loads(text)
        except:
            pass

        # å°è¯•2: æå– markdown ä»£ç å—ä¸­çš„ JSON
        # ```json {...} ``` æˆ– ```{...}```
        match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass

        # å°è¯•3: æå–ç¬¬ä¸€ä¸ª {...} å—
        match = re.search(r'\{.*?\}', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(0))
            except:
                pass

        # å°è¯•4: å¦‚æœåŒ…å« "same" å’Œ "confidence" å…³é”®å­—ï¼Œå°è¯•æ‰‹åŠ¨è§£æ
        if "same" in text.lower() and "confidence" in text.lower():
            same = "true" in text.lower() or '"same": true' in text.lower()

            # æå– confidence æ•°å€¼
            conf_match = re.search(r'"confidence"\s*:\s*([\d.]+)', text)
            confidence = float(conf_match.group(1)) if conf_match else (0.5 if same else 0.0)

            return {"same": same, "confidence": confidence}

        raise ValueError(f"Could not extract JSON from response: {text[:200]}")

    def merge_entities(
            self,
            df: pl.DataFrame,
            merge_graph: Dict[int, int]
    ) -> pl.DataFrame:
        """
        åº”ç”¨åˆå¹¶
        """

        if not merge_graph:
            return df

        # ä¸ºæ¯è¡Œåˆ†é…cluster ID
        cluster_ids = []
        for i in range(len(df)):
            cluster_ids.append(self.find_root(merge_graph, i))

        df_with_cluster = df.with_columns([
            pl.Series("_cluster_id", cluster_ids)
        ])

        # æŒ‰clusterèšåˆ
        # ç­–ç•¥: æ•°å€¼å–å¹³å‡ï¼Œæ–‡æœ¬å–é¦–ä¸ªéç©º
        agg_exprs = []
        for col in df.columns:
            if df[col].dtype.is_numeric():
                agg_exprs.append(pl.col(col).mean().alias(col))
            else:
                agg_exprs.append(
                    pl.col(col).filter(pl.col(col).is_not_null()).first().alias(col)
                )

        resolved = df_with_cluster.group_by("_cluster_id").agg(agg_exprs)
        resolved = resolved.drop("_cluster_id")

        return resolved

    def generate_report(
            self,
            original_df: pl.DataFrame,
            resolved_df: pl.DataFrame,
            merge_graph: Dict[int, int]
    ) -> pl.DataFrame:
        """
        ç”ŸæˆERæŠ¥å‘Š
        """

        # ç»Ÿè®¡æ¯ä¸ªclusteråˆå¹¶äº†å¤šå°‘è¡Œ
        cluster_sizes = {}
        for idx in range(len(original_df)):
            root = self.find_root(merge_graph, idx)
            cluster_sizes[root] = cluster_sizes.get(root, 0) + 1

        report_data = []
        for cluster_id, size in cluster_sizes.items():
            if size > 1:  # åªæŠ¥å‘Šå®é™…åˆå¹¶çš„
                report_data.append({
                    "cluster_id": cluster_id,
                    "records_merged": size
                })

        if not report_data:
            return pl.DataFrame({
                "message": ["No duplicates found"]
            })

        report = pl.DataFrame(report_data).sort("records_merged", descending=True)
        return report